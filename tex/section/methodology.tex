\section{Methodology}
\label{sec:methods}
For each of our experiments we followed a common pipeline, shown in \autoref{fig:pipeline}.

\begin{figure}[htb!]
    \centering
    \begin{tikzpicture}[
        node distance=1.8cm and 0.6cm,
        every node/.style={font=\small},
        box/.style={draw, rectangle, rounded corners, minimum height=1cm, minimum width=2.6cm, align=center},
        group/.style={draw, rectangle, dashed, inner sep=0.3cm, rounded corners}
    ]

    \node[box] (features) {Features \\ (SNP Matrix)};
    \node[box, below=of features] (annotation) {Functional \\ Annotations};
    \node[box, below=of annotation] (mortality) {Mortality \\ Labels};

    \node[group, fit=(features)(mortality)(annotation), label=above:{\textbf{Input Datasets}}] (dataset_group) {};

    \node[box, right=1.5cm of features] (uniform) {Uniform};
    \node[box, below=1cm of uniform] (chr) {Per-Chromosome};
    \node[box, below=1cm of chr] (annot) {Annotation-Based};

    \node[group, fit=(uniform)(chr)(annot), label=above:{\textbf{Subsampling}}] (subsampling_group) {};

    \node[box, right=1.5cm of uniform] (split) {Train/Test Split};
    \node[box, right=1.5cm of split] (xgboost) {XGBoost Training};
    \node[box, below=of xgboost] (evaluate) {Evaluation};

    \draw[->] (features.east) -- ++(0.4,0) |- (uniform.west);
    \draw[->] (features.east) -- (chr.west);
    \draw[->] (features.east) -- (annot.west);

    \draw[->] (annotation.east) -- ++(0.7,0) |- (annot.west);

    \draw[->] (uniform.east) -- ++(0.4,0) |- (split.west);
    \draw[->] (chr.east) -- (split.west);
    \draw[->] (annot.east) -- (split.west);

    \draw[->, thick] (mortality.east) -| (split.south);

    \draw[->] (split.east) -- (xgboost.west);
    \draw[->] (split.east) -- (evaluate.west);
    \draw[->] (xgboost.south) -- (evaluate.north);

    \end{tikzpicture}
    \caption{Pipeline of the model training after subsampling of the data}
    \label{fig:pipeline}
\end{figure}

In the following part we'll describe the details.


\subsection{Dataset}

The main dataset we used was a table characterising SNPs of $990$ individual sea basses, each sea bass having $6072853$ nucleotides included. We refer to it as the \textbf{features} dataset.

Each row of the features dataset corresponds to a sea bass, and each column corresponds to a specific nucleotide in the genome.
The values in the table represent SNPs. Each value can be a number from 0 to 2, representing the number of alleles mutated for that particular sea bass in that specific nucleotide position:
\begin{itemize}
    \item 0: no mutation (homozygous reference),
    \item 1: one allele mutated (heterozygous),
    \item 2: both alleles mutated (homozygous alternate).
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Example rows from the SNP features dataset. Each cell denotes a genotype value at a specific nucleotide.}
    \label{tab:features_example}
    \begin{tabular}{l|cccc}
        \textbf{features} & CAJNNU010000001.1:299 & CAJNNU010000001.1:903 & CAJNNU010000001.1:986 & $\dots$ \\
        \hline
        PL04-A06 & 0 & 1 & 2 & $\dots$ \\
        PL04-A07 & 1 & 0 & 1 & $\dots$ \\
        PL04-A08 & 2 & 2 & 0 & $\dots$ \\
        $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$
    \end{tabular}
\end{table}

Of each nucleotide we know the chromosome that it belongs to.

Paired with the features dataset we have a \textbf{mortality} dataset. 
It stores for each sea bass if it died or not after the contraption of VNN.

In addition to these two datasets, we had a set of \textbf{annotated} nucleotides.
These are special nucleotides for which further information is known on their role in the sea bass genome.
This information comes in the form of a function (either \texttt{Open\_chromatin}, \texttt{Enhancer} or \texttt{Promoter}) and a tissue number (between $0$ and $25$).


\subsection{Subsampling}

The subsampling is done over the nucleotides, meaning that each individual remains in the dataset but we consider only a random subset of its nucleotides.

We introduce the \textbf{subsampling ratio} $p$ parameter, which just specifies the ratio of nucleotides to be randomly selected over the available ones.

We then find three main subsampling techniques to apply to our features dataset.

\paragraph{Uniform subsampling.}
The first and simplest thing we tried was to randomly and uniformly subsample the nucleotides on the entire genome.
Given $p$, we find the number of nucleotides to be subsampled by multiplying $p$ by the number of nucleotides, rounding down.
Then we uniformly subsample that number of nucleotides.

\paragraph{Uniform subsampling on chromosomes.}
One observation that could be made with the previous method is that it could select a really high number of nucleotides present in a chromosome while next to no one from another. 
So the importance that chromosomes might have in the information given to the learning model is not captured. 
We can constrain the subsampling to take from each chromosome the same number of nucleotides uniformly at random, getting a subsampling tecnique that balances each chromosome contribution in the training set.

\paragraph{Subsampling using annotations.}
We could also make use of the further informations that we have on the annotated nucleotides.
Specifically, we restrict the pool of SNPs to those with certain annotations (e.g., only \texttt{Enhancer} regions) or within a desired tissue number range. From this filtered pool, we then apply uniform subsampling as before.


\subsection{Training and testing}

After the subsampling we need to train and test our model on the resulting data. 
We split the population in two sets (training and testing), then we train xgboost on the training set and find out if the resulting model has predictive power on the test set.

The individuals used for training and testing are fixed before subsampling the nucleotides. This is made in order to reduce the statistical perturbation that might come from another random procedure.
We also fixed the seed of XGBoost, although we don't expect this to make any change on the variance, as the inputs given to the model change with every subsampling.

Since we are dealing with random processes, for each subsampling method and ratio we ran and evaluated multiple instances, taking mean an variance of the scores.